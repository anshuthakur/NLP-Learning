{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF \n",
    "It stands for Term Frequency - Inverse Document Frequency. It is a way to quantify words in a document. It's structure seems somewhat similar to that of Count Vectorizer's vectors, but there are certain differences. Most important one is that Count Vectorizer basically gives us a count of a certain word in the document, while Tf-Idf will give us weights which show how important a word is. The TF part will tell us how frequent the word is in a document, while the IDF part will give us the invers of how frequent a word is across the documents. It is based on the idea that certain words - like 'the', 'is', 'a' and so on, while frequent, don't add much to the meaning of the document. They are prevalent across all documents. Whereas certain terms like  - \"spacecraft\", technical terms and so on, while they're popular across one document, they are not going to be there in all the documents.\n",
    "\n",
    "TFIDF is most useful for retrieving information. \n",
    "\n",
    "So let's get started with this, and get our document corpus first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "from random import shuffle\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "import math\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the corpus\n",
    "with open(\"sample text.txt\",\"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# get the sentences\n",
    "corpus = content.split(\"\\n\")\n",
    "corpus = [c for c in corpus if len(c.strip()) > 0]\n",
    "shuffle(corpus)\n",
    "shuffle(corpus)\n",
    "shuffle(corpus)\n",
    "shuffle(corpus)\n",
    "shuffle(corpus)\n",
    "corpus = corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clean the sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean sents and tokenize\n",
    "def bag_of_words(s):\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    tokens = [token.lower() for token in tokens if re.findall(r\"\\w\", token)]\n",
    "    tokens = [token.strip().strip('.').strip(\"—\").strip(\"'\") for token in tokens]\n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the unique words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the unique words\n",
    "def get_vocab(all_text):\n",
    "    tokenized_text = [bag_of_words(s) for s in all_text]\n",
    "    tokens = [token for tokens in tokenized_text for token in tokens]\n",
    "    tokens = list(set(tokens))\n",
    "    tokens.sort()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the term frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tf(doc, unique_words):\n",
    "    tf_dict = dict()\n",
    "    tokens = bag_of_words(doc)\n",
    "    tf = [0]*len(unique_words)\n",
    "    N = len(tokens)\n",
    "    for token in tokens:\n",
    "        tf_dict[token] = tf_dict.setdefault(token, 0)+1\n",
    "    \n",
    "    for word in tf_dict:\n",
    "        tf_dict[word] = tf_dict[word] / N\n",
    "    for i, w in enumerate(unique_words):\n",
    "        if w in tf_dict:\n",
    "            tf[i] = tf_dict[w]\n",
    "        \n",
    "    return tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the Term Frequencies for all the terms for all the documents in the corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"I will be there, you may be sure.— MAUDIE.\"\n",
      "\"You are funny, Masser Holmes, ain't you?\"\n",
      "Story 54, Case 53, Collier's Weekly, November 8, 1924;\n",
      "\"It was just a week ago to-day. The creature was howling outside the old well-house, and Sir Robert was in one of his tantrums that morning. He caught it up, and I thought he would have killed it. Then he gave it to Sandy Bain, the jockey, and told him to take the dog to old Barnes at the Green Dragon, for he never wished to see it again.\"\n",
      "\"Yet you say he is affectionate?\"\n",
      "\"'I have his letters to me in my pocket.'\n",
      "\"Dear me, Holmes!\" I cried, \"that seemed to me to be the most damning incident of all.\"\n",
      "\"No, I heard nothing. But, indeed, Mr. Holmes, I was so agitated and horrified by this terrible outbreak that I rushed to get back to the peace of my own room, and I was incapable of noticing anything which happened.\"\n",
      "\"But surely,\" said I, \"the vampire was not necessarily a dead man? A living person might have the habit. I have read, for example, of the old sucking the blood of the young in order to retain their youth.\"\n",
      "\"Your own question might perhaps come under the same heading.\"\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2222222222222222, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0.1111111111111111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0, 0, 0, 0.1111111111111111, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "# computing TF\n",
    "vocabulary = get_vocab(corpus)\n",
    "TF_vals = [compute_tf(para, vocabulary) for para in corpus]\n",
    "print(*corpus, sep=\"\\n\")\n",
    "print(TF_vals[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_df(all_text):\n",
    "    tokenized_text = [bag_of_words(doc) for doc in all_text]\n",
    "    unique_words = get_vocab(all_text)\n",
    "    df = dict.fromkeys(unique_words, 0)\n",
    "    for i,tokens in enumerate(tokenized_text):\n",
    "        for token in tokens:\n",
    "            for w in unique_words:\n",
    "                if token == w:\n",
    "                    df[token] += 1\n",
    "                    break\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "# computing document frequency\n",
    "df_val = compute_df(corpus)\n",
    "from pprint import pprint\n",
    "pprint(df_val[\"a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_idf(all_text):\n",
    "    word_freq = compute_df(all_text)\n",
    "    idf = dict()\n",
    "    N = len(all_text)\n",
    "    for word in word_freq:\n",
    "        idf[word] = np.log(N/(word_freq[word] + 1))\n",
    "    return idf\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9162907318741551"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf_vals = compute_idf(corpus)\n",
    "idf_vals['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### compute tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_tfidf(all_text, TF_VAL):\n",
    "    unique_words = get_vocab(all_text)\n",
    "    tf_vals = [compute_tf(text,unique_words) for text in all_text]\n",
    "    idf_vals = compute_idf(all_text)\n",
    "#     print(tf_vals == TF_VAL)\n",
    "\n",
    "    tf_idf = list()\n",
    "    \n",
    "    for i in range(len(all_text)):\n",
    "        temp = list()\n",
    "        for j, w in enumerate(unique_words):\n",
    "            \n",
    "            temp.append(tf_vals[i][j] * idf_vals[w])\n",
    "#         print(temp)\n",
    "        tf_idf.append(temp)\n",
    "#     print()\n",
    "#     print(*tf_idf,sep =\"\\n\")\n",
    "    return tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ti_vals = compute_tfidf(corpus, TF_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.2036201626387011, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.01059001997825832, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17882643471490003, 0.17882643471490003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17882643471490003, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.0, 0.0, 0.0, 0.17882643471490003, 0.0, 0.0, -0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.17882643471490003, 0.0, 0.0, 0.0, 0.07701635339554948, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "print(ti_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.2222222222222222, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0.1111111111111111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0.1111111111111111, 0, 0, 0, 0.1111111111111111, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(TF_vals[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
