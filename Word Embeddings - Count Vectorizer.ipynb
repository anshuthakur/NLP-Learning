{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "Natural language processing is a subfield of AI that deals with natural languages. Most of the tasks that come under NLP can be generally grouped into two categories - understanding natural language, or generating natural language.\n",
    "\n",
    "NLU or Natural Language Understanding involves tasks where one basically reads the text and tries to do something with it - like classifying it, extracting information from it. While the NLG or Natural Language Generation involves any task where the computer itself has to create an output, like in machine translation. Of course, any particular task that we have in NLP won't be strictly NLU or NLG, it could be a mix of the two.\n",
    "\n",
    "Some of challenges associated while working with Natural languages are 1) the ambiguity in languages. The human languages are not really clear a lot of times, as in we ourselves face the difficulty in understanding what is happening, so it is not out of the realm of possibility to assume that the computers will face this issue as well. And 2) how to feed in words to a deep learning / machine learning model. These models are mathematical in nature and can process only numbers. So, in order to make the computer understand these languages, we will have to convert our language (made of words) into numbers.\n",
    "\n",
    "This numeric representation of words is called as word embeddings.\n",
    "\n",
    "In this notebook we will look at one of the most simple ways to vectorize our text - Count Vectorizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is kind of simple.\n",
    "1. Get the text corpus (our term for a list of sentences. these sentences might also be called as documents).\n",
    "2. Tokenize the documents in the corpus. (basically, send each sentence/doc through a tokenizer and get individual word/tokens, each token will have its own place in the vector space, that is they are all going to be one feature. think of a row vector where each token is a word that you have tokenized).\n",
    "3. Get the list of unique words across the corpus. This is going to be the size of our vector space.\n",
    "4. Consider each sentence/doc in the corpus, and basically count the occurence of each token in the sentence/doc and get a row vector representing that count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import spacy\n",
    "from random import shuffle\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the corpus\n",
    "with open(\"sample text.txt\",\"r\") as f:\n",
    "    content = f.read()\n",
    "\n",
    "# get the sentences\n",
    "corpus = nltk.sent_tokenize(content)\n",
    "shuffle(corpus)\n",
    "shuffle(corpus)\n",
    "shuffle(corpus)\n",
    "shuffle(corpus)\n",
    "shuffle(corpus)\n",
    "corpus = corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean sents and tokenize\n",
    "def bag_of_words(s):\n",
    "    tokenizer = RegexpTokenizer(\"\\w+\")\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    tokens = [token.lower() for token in tokens if re.findall(r\"\\w\", token)]\n",
    "    tokens = [token.strip().strip('.').strip(\"â€”\").strip(\"'\") for token in tokens]\n",
    "    return tokens\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the vocabulary\n",
    "def get_vocab(all_text):\n",
    "    tokenized_text = [bag_of_words(s) for s in all_text]\n",
    "    tokens = [token for tokens in tokenized_text for token in tokens]\n",
    "    tokens = list(set(tokens))\n",
    "    tokens.sort()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the count_vect(s, unique_words)\n",
    "def get_count_vect(s, vocab):\n",
    "    bagofwords = bag_of_words(s)\n",
    "    cv = [0]*len(vocab)\n",
    "    for w in bagofwords:\n",
    "        for i,sw in enumerate(vocab):\n",
    "            if w == sw:\n",
    "                cv[i] += 1\n",
    "                break\n",
    "    return cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = get_vocab(corpus)\n",
    "cv = [get_count_vect(s, vocabulary) for s in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A pathetic, futile, broken creature.\"\n",
      "('a', 1)\n",
      "('about', 0)\n",
      "('against', 0)\n",
      "('an', 0)\n",
      "('and', 0)\n",
      "('aware', 0)\n",
      "('be', 0)\n",
      "('before', 0)\n",
      "('brave', 0)\n",
      "('broke', 0)\n",
      "('broken', 1)\n",
      "('but', 0)\n",
      "('can', 0)\n",
      "('chance', 0)\n",
      "('creature', 1)\n",
      "('day', 0)\n",
      "('distant', 0)\n",
      "('down', 0)\n",
      "('enough', 0)\n",
      "('find', 0)\n",
      "('futile', 1)\n",
      "('going', 0)\n",
      "('have', 0)\n",
      "('he', 0)\n",
      "('him', 0)\n",
      "('how', 0)\n",
      "('i', 0)\n",
      "('in', 0)\n",
      "('is', 0)\n",
      "('knew', 0)\n",
      "('lake', 0)\n",
      "('let', 0)\n",
      "('listening', 0)\n",
      "('machine', 0)\n",
      "('man', 0)\n",
      "('mcpherson', 0)\n",
      "('might', 0)\n",
      "('more', 0)\n",
      "('mr', 0)\n",
      "('music', 0)\n",
      "('old', 0)\n",
      "('pathetic', 1)\n",
      "('said', 0)\n",
      "('shoscombe', 0)\n",
      "('some', 0)\n",
      "('strong', 0)\n",
      "('that', 0)\n",
      "('the', 0)\n",
      "('through', 0)\n",
      "('to', 0)\n",
      "('urgent', 0)\n",
      "('us', 0)\n",
      "('voice', 0)\n",
      "('was', 0)\n",
      "('well', 0)\n",
      "('were', 0)\n",
      "('what', 0)\n",
      "('with', 0)\n",
      "('woman', 0)\n",
      "('you', 0)\n",
      "('yourself', 0)\n"
     ]
    }
   ],
   "source": [
    "print(corpus[5])\n",
    "print(*zip(vocabulary, cv[5]),sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
